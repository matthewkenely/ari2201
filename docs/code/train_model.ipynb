{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('location_articles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(df.body)\n",
    "Y = list(df.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "Y_ready = []\n",
    "\n",
    "for sen_tags in Y:\n",
    "    Y_ready.append(literal_eval(sen_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = Y_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = ['O', 'LOC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cutoff reviews after 110 words\n",
    "maxlen = 110\n",
    "\n",
    "# consider the top 36000 words in the dataset\n",
    "max_words = 36000\n",
    "\n",
    "# tokenize each sentence in the dataset\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10489 unique tokens.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([   1, 1826,  623,    4,  346,  339,    1,  922,  253,   29, 8095,\n",
       "         18,    6,  689,  436,  999,  143,   49,  926,    7,    6,  421,\n",
       "          1,   47,   16,    3,    6, 2740,   84,   21, 2575,    8, 1574,\n",
       "         41,    1,  689,    9,    8,  658, 3304,   18,    6, 5654, 5655,\n",
       "          5,    6, 8096, 8097,    6,  970,  260,    8,   32, 2115,    7,\n",
       "          1,  922,   27, 1423,   47,  575,    5, 1417,   83,  903,   22,\n",
       "         10,    1,  941,    2, 1250,    1, 8098,  230,    7,    1,  922,\n",
       "          1, 1034,    8,  159,  731,    2,   40,  719,  950,  312,   20,\n",
       "          6,  960,    4,    1,  922, 1769,  692, 1780,   93,   18,  193,\n",
       "       2251,    4,  643, 3377,    2,    1, 1911,  256,    7,  226, 2973])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = tokenizer.word_index\n",
    "print(\"Found {} unique tokens.\".format(len(word_index)))\n",
    "ind2word = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "word2id = word_index\n",
    "\n",
    "# dict. that map each identifier to its word\n",
    "id2word = {}\n",
    "for key, value in word2id.items():\n",
    "    id2word[value] = key\n",
    "\n",
    "# pad the sequences so that all sequences are of the same size\n",
    "X_preprocessed = pad_sequences(sequences, maxlen=maxlen, padding='post')\n",
    "\n",
    "# first example after tokenization and padding. \n",
    "X_preprocessed[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict. that map each tag to its identifier\n",
    "tags2id = {}\n",
    "for i, tag in enumerate(tags):\n",
    "    tags2id[tag] = i\n",
    "\n",
    "# dict. that map each identifier to its tag\n",
    "id2tag = {}\n",
    "for key, value in tags2id.items():\n",
    "    id2tag[value] = key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_tags(tags2id, Y_ready):\n",
    "    \n",
    "    Y_preprocessed = []\n",
    "    maxlen = 110\n",
    "    # for each target \n",
    "    for y in Y_ready:\n",
    "        \n",
    "        # place holder to store the new preprocessed tag list\n",
    "        Y_place_holder = []\n",
    "        \n",
    "        # for each tag in rhe tag list \n",
    "        for tag in y:\n",
    "            # append the id of the tag in the place holder list\n",
    "            Y_place_holder.append(tags2id[tag])\n",
    "        \n",
    "        # find the lenght of the new preprocessed tag list \n",
    "        len_new_tag_list = len(Y_place_holder)\n",
    "        # find the differance in length between the len of tag list and padded sentences\n",
    "        num_O_to_add = maxlen - len_new_tag_list\n",
    "        \n",
    "        # add 'O's to padd the tag lists\n",
    "        padded_tags = Y_place_holder + ([tags2id['O']] * num_O_to_add)\n",
    "        Y_preprocessed.append(padded_tags)\n",
    "        \n",
    "    return Y_preprocessed\n",
    "\n",
    "Y_preprocessed = preprocess_tags(tags2id, Y_ready)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Matthew Kenely\\AppData\\Roaming\\Python\\Python37\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "X_preprocessed = np.asarray(X_preprocessed)\n",
    "Y_preprocessed = np.asarray(Y_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_preprocessed, Y_preprocessed, train_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21616\\3726424813.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mval_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, Y_val))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
